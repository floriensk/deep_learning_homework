{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install tqdm\n",
    "%pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data fetching\n",
    "We use a streaming solution to fetch data, this way we are able to track progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_file(uri, target_path):\n",
    "    # Create directory path to target file\n",
    "    if not os.path.exists(os.path.dirname(target_path)):\n",
    "        os.makedirs(os.path.dirname(target_path))\n",
    "\n",
    "    # Download file using streaming, so we can iterate over the response\n",
    "    response = requests.get(uri, stream=True)\n",
    "    total_size_in_bytes= int(response.headers.get('content-length', 0)) # Total size of data to download\n",
    "    block_size = 1024 # Download in chunks for progress tracking\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True) # Use a progress bar to track progress\n",
    "\n",
    "    with open(target_path, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data) # Write downloaded chunk to file\n",
    "    progress_bar.close()\n",
    "\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(f\"Error during download of {target_path}\")\n",
    "    else:\n",
    "        print(f\"Downloading {target_path} finished successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we fetch the images from the corresponding Google Drive folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../data/fairface\" # Path of directory to extract into\n",
    "\n",
    "uri_images = \"https://drive.google.com/uc?export=download&id=1g7qNOZz9wC7OfOhcPqH1EZ5bk1UFGmlL&confirm=t&uuid=729c215d-4fa4-4799-b03f-aea00a016230&at=ALAFpqx7EciTPuBT0YNhhbYsVpML:1666561770553\"\n",
    "images_file_path = \"../data/fairface.zip\" # Path of downloaded ZIP file\n",
    "\n",
    "download_file(uri_images, images_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fetch the CSV files containing the labels for the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_labels_train = \"https://drive.google.com/uc?export=download&id=1i1L3Yqwaio7YSOCj7ftgk8ZZchPG7dmH\"\n",
    "labels_train_valid_file_path = os.path.join(dir_path, \"labels_train_valid.csv\") # Will be split into train and valid, so already naming it that way\n",
    "\n",
    "download_file(uri_labels_train, labels_train_valid_file_path)\n",
    "\n",
    "uri_labels_val = \"https://drive.google.com/uc?export=download&id=1wOdja-ezstMEp81tX1a-EYkFebev4h7D\"\n",
    "labels_test_file_path = os.path.join(dir_path, \"labels_test.csv\") # Will be used as test set, so already naming it that way\n",
    "\n",
    "download_file(uri_labels_val, labels_test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction\n",
    "The data needs to be uncompressed. Then the labels for the training are extracted from the file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(images_file_path) as zip:\n",
    "    zip.extractall(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete ZIP after extracting\n",
    "os.remove(images_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read the labels into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels_train_valid = np.loadtxt(labels_train_valid_file_path, delimiter=\",\", skiprows=1, dtype=\"str\") # Read while skipping header\n",
    "labels_test = np.loadtxt(labels_test_file_path, delimiter=\",\", skiprows=1, dtype=\"str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data segmentation\n",
    "Finally, we split the data into train, validation and test datasets for further use by our model.\n",
    "\n",
    "Data in the downloaded dataset is already split into *train* and *val* subsets (the latter makes up about 10% of all images). Since we need to split the dataset into train, validation and test subsets, we will turn the specified *val* subset into the test subset and split the specified *train* subset into train and validation subsets.\n",
    "\n",
    "The resulting split ratios are as follows:\n",
    "+ train: ~74%\n",
    "+ validation: ~15%\n",
    "+ test: ~11%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn \"train\" into \"train_valid\"\n",
    "dir_train_valid_path = os.path.join(dir_path, \"train_valid\")\n",
    "os.rename(os.path.join(dir_path, \"train\"), dir_train_valid_path)\n",
    "\n",
    "# Turn \"val\" into \"test\"\n",
    "dir_test_path = os.path.join(dir_path, \"test\")\n",
    "os.rename(os.path.join(dir_path, \"val\"), dir_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_valid_len = len(os.listdir(dir_train_valid_path))\n",
    "validation_size = (train_valid_len + len(os.listdir(dir_test_path))) * 0.15 / train_valid_len # 15% of all images -> ?% of images in \"train_valid\"\n",
    "\n",
    "labels_train, labels_valid = train_test_split(labels_train_valid, test_size = validation_size, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully created the three subsets, *train*, *valid* and *test*. (Note that in the file system, only test is in a separate directory, as it was in the original database that way. Separating the other subsets would be an unnecessary operation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 72089 images (73.8%)\n",
      "valid: 14655 images (15.0%)\n",
      "test:  10954 images (11.2%)\n"
     ]
    }
   ],
   "source": [
    "train_images_count = len(labels_train)\n",
    "valid_images_count = len(labels_valid)\n",
    "test_images_count = len(labels_test)\n",
    "images_count = train_images_count + valid_images_count + test_images_count\n",
    "\n",
    "print(f\"train: {train_images_count} images ({train_images_count / images_count * 100:.1f}%)\")\n",
    "print(f\"valid: {valid_images_count} images ({valid_images_count / images_count * 100:.1f}%)\")\n",
    "print(f\"test:  {test_images_count} images ({test_images_count / images_count * 100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
