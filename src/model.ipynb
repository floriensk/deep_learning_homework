{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning homework\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/floriensk/deep_learning_homework/blob/main/src/model.ipynb\">\n",
    "<button>\n",
    "Open in Colab\n",
    "</button>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install tqdm\n",
    "%pip install sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data fetching\n",
    "We use the *fairface* dataset to train our model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below switch allows to use the **dataset** from the **local** filesystem instead of **downloading** it from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LOCAL_DATASET = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a streaming solution to fetch data, this way we are able to track progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_file(uri, target_path):\n",
    "    # Create directory path to target file\n",
    "    if not os.path.exists(os.path.dirname(target_path)):\n",
    "        os.makedirs(os.path.dirname(target_path))\n",
    "\n",
    "    # Download file using streaming, so we can iterate over the response\n",
    "    response = requests.get(uri, stream=True)\n",
    "    total_size_in_bytes= int(response.headers.get('content-length', 0)) # Total size of data to download\n",
    "    block_size = 1024 # Download in chunks for progress tracking\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True) # Use a progress bar to track progress\n",
    "\n",
    "    with open(target_path, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data) # Write downloaded chunk to file\n",
    "    progress_bar.close()\n",
    "\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        print(f\"Error during download of {target_path}\")\n",
    "    else:\n",
    "        print(f\"Downloading {target_path} finished successfully\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we fetch the images from the corresponding Google Drive folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../data\" # Path of directory to extract the downloaded data into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_LOCAL_DATASET:\n",
    "    uri_images = \"https://drive.google.com/uc?export=download&id=1g7qNOZz9wC7OfOhcPqH1EZ5bk1UFGmlL&confirm=t&uuid=729c215d-4fa4-4799-b03f-aea00a016230&at=ALAFpqx7EciTPuBT0YNhhbYsVpML:1666561770553\"\n",
    "    images_file_path = \"../data/fairface.zip\" # Path of downloaded ZIP file\n",
    "\n",
    "    download_file(uri_images, images_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncompressing the images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images need to be uncompressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_LOCAL_DATASET:\n",
    "    from zipfile import ZipFile\n",
    "\n",
    "    with ZipFile(images_file_path) as zip:\n",
    "        zip.extractall(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete ZIP after extracting\n",
    "if not USE_LOCAL_DATASET:\n",
    "    os.remove(images_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fetch the CSV files containing the labels for the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_valid_file_path = os.path.join(dir_path, \"labels_train_valid.csv\") # Will be split into train and valid, so already naming it that way\n",
    "\n",
    "labels_test_file_path = os.path.join(dir_path, \"labels_test.csv\") # Will be used as test set, so already naming it that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_LOCAL_DATASET:\n",
    "    uri_labels_train = \"https://drive.google.com/uc?export=download&id=1i1L3Yqwaio7YSOCj7ftgk8ZZchPG7dmH\"\n",
    "    download_file(uri_labels_train, labels_train_valid_file_path) # Download train and valid data sets\n",
    "\n",
    "    uri_labels_val = \"https://drive.google.com/uc?export=download&id=1wOdja-ezstMEp81tX1a-EYkFebev4h7D\"\n",
    "    download_file(uri_labels_val, labels_test_file_path) # Download test data set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, read the labels into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels_train_valid = np.loadtxt(labels_train_valid_file_path, delimiter=\",\", skiprows=1, dtype=\"str\") # Read while skipping header\n",
    "labels_test = np.loadtxt(labels_test_file_path, delimiter=\",\", skiprows=1, dtype=\"str\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data segmentation\n",
    "Finally, we split the data into train, validation and test datasets for further use by our model.\n",
    "\n",
    "Data in the downloaded dataset is already split into *train* and *val* subsets (the latter makes up about 10% of all images). Since we need to split the dataset into train, validation and test subsets, we will turn the specified *val* subset into the test subset and split the specified *train* subset into train and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_train_valid_path = os.path.join(dir_path, \"train_valid\")\n",
    "dir_test_path = os.path.join(dir_path, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the extracted folders accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_LOCAL_DATASET:\n",
    "    # Turn \"train\" into \"train_valid\"\n",
    "    os.rename(os.path.join(dir_path, \"train\"), dir_train_valid_path)\n",
    "    # Turn \"val\" into \"test\"\n",
    "    os.rename(os.path.join(dir_path, \"val\"), dir_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import tensorflow\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracts the file name from a file path\n",
    "def get_file_name(file_path):\n",
    "    split_path = tensorflow.strings.split(file_path, \"\\\\\").numpy()\n",
    "    split_path = np.atleast_2d(split_path)[:,-1]\n",
    "    split_path = tensorflow.strings.split(split_path, \"/\").numpy()\n",
    "    split_path = np.atleast_2d(split_path)[:,-1]\n",
    "    return split_path\n",
    "\n",
    "## Returns the image as a numpy array from a file path\n",
    "def get_image(file_path):\n",
    "    return tensorflow.keras.utils.img_to_array(\n",
    "        tensorflow.keras.utils.load_img(file_path)\n",
    "    )\n",
    "\n",
    "## Returns the labels for a file path\n",
    "## @param test_dataset: if True, the test dataset will be used, otherwise the train and valid dataset\n",
    "def get_labels(file_path, test_dataset=False):\n",
    "    file_name = get_file_name(file_path) # get only the file name, without a path\n",
    "\n",
    "    labels = labels_test if test_dataset else labels_train_valid # get the desired set of labels\n",
    "    \n",
    "    return labels[get_file_name(labels[:, 0]) == file_name].ravel()[1:] # get the labels for the file name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We one-hot encode all labels separately, then construct the desired output of the model by concatenating these arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode labels\n",
    "labels_all = np.append(labels_train_valid, labels_test, axis=0)\n",
    "\n",
    "age_encoder = preprocessing.OneHotEncoder(sparse=False, dtype=\"float32\")\n",
    "age_encoder.fit(labels_all[:, 1].reshape(-1, 1))\n",
    "\n",
    "gender_encoder = preprocessing.OneHotEncoder(sparse=False, dtype=\"float32\")\n",
    "gender_encoder.fit(labels_all[:, 2].reshape(-1, 1))\n",
    "\n",
    "race_encoder = preprocessing.OneHotEncoder(sparse=False, dtype=\"float32\")\n",
    "race_encoder.fit(labels_all[:, 3].reshape(-1, 1))\n",
    "\n",
    "## Converts the labels to the output of the model\n",
    "def encode_labels(labels):\n",
    "    labels = np.atleast_2d(labels)\n",
    "\n",
    "    age_labels = age_encoder.transform(np.atleast_2d(labels[:, 0]))\n",
    "    gender_labels = gender_encoder.transform(np.atleast_2d(labels[:, 1]))\n",
    "    race_labels = race_encoder.transform(np.atleast_2d(labels[:, 2]))\n",
    "    \n",
    "    return np.append( # append age_labels + gender_labels + race_labels\n",
    "        np.append(\n",
    "            age_labels,\n",
    "            gender_labels,\n",
    "            axis=1\n",
    "            ),\n",
    "        race_labels,\n",
    "        axis=1\n",
    "        ).reshape(-1)\n",
    "\n",
    "## Converts the output of the model to the decoded labels\n",
    "def decode_labels(labels):\n",
    "    labels = np.atleast_2d(labels)\n",
    "\n",
    "    age_labels = age_encoder.inverse_transform(labels[:, :9])\n",
    "    gender_labels = gender_encoder.inverse_transform(labels[:, 9:11])\n",
    "    race_labels = race_encoder.inverse_transform(labels[:, 11:])\n",
    "\n",
    "    return np.append( # append the decoded labels\n",
    "        np.append(\n",
    "            age_labels,\n",
    "            gender_labels,\n",
    "            axis=1\n",
    "        ),\n",
    "        race_labels,\n",
    "        axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maps a file path to the image and the labels\n",
    "def get_datapoint(file_path, test_dataset=False):\n",
    "    # file_path = file_path.numpy() # convert tensor to value\n",
    "    image = get_image(file_path)\n",
    "    labels = encode_labels(get_labels(file_path, test_dataset))\n",
    "    \n",
    "    return (\n",
    "        image,\n",
    "        labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file paths from the train and valid dataset\n",
    "dataset_train_valid = Dataset.list_files(f\"{dir_train_valid_path}/*\", shuffle=True, name=\"dataset_train_valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the file paths in the dataset to the actual image and label data\n",
    "dataset_train_valid = dataset_train_valid.map(\n",
    "    # using py_function to enable eager execution and thus the use of numpy in the map function\n",
    "    lambda file_path: tensorflow.numpy_function(\n",
    "        get_datapoint,\n",
    "        inp=[file_path],\n",
    "        Tout=(tensorflow.float32, tensorflow.float32),\n",
    "        stateful=False\n",
    "    ),\n",
    "    num_parallel_calls=tensorflow.data.experimental.AUTOTUNE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns the shapes of the input and output of the model\n",
    "def get_datapoint_shapes():\n",
    "    sample = dataset_train_valid.as_numpy_iterator().next()\n",
    "    return sample[0].shape, sample[1].shape # trim batch dimension\n",
    "\n",
    "# Save the shapes of the input and output of the model\n",
    "input_shape, output_shape = get_datapoint_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of the use of py_function, the shapes of the input and output are not known for the tensorflow graph builder,\n",
    "# so we have to reshape the input and output to the correct shape\n",
    "dataset_train_valid = dataset_train_valid.map(\n",
    "    lambda image, labels: (\n",
    "        tensorflow.reshape(image, input_shape),\n",
    "        tensorflow.reshape(labels, output_shape)\n",
    "    ),\n",
    "    num_parallel_calls=tensorflow.data.experimental.AUTOTUNE,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the first data set into train and validation data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ratio = 0.8\n",
    "batch_size = 32\n",
    "\n",
    "data_count_train = int(len(labels_train_valid) * train_data_ratio)\n",
    "data_count_valid = len(labels_train_valid) - data_count_train\n",
    "\n",
    "# split the dataset into train and validation data\n",
    "dataset_train = dataset_train_valid.take(data_count_train, name=\"dataset_train\")\n",
    "dataset_valid = dataset_train_valid.skip(data_count_train, name=\"dataset_valid\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, prepare the datasets to be an appropriate source for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset):\n",
    "    return (dataset\n",
    "        .repeat()\n",
    "        .batch(\n",
    "            batch_size,\n",
    "            num_parallel_calls=tensorflow.data.experimental.AUTOTUNE,\n",
    "            deterministic=False,\n",
    "            name=\"dataset_batch\"\n",
    "        ).prefetch(tensorflow.data.experimental.AUTOTUNE, name=\"dataset_prefetch\"))\n",
    "\n",
    "\n",
    "dataset_train = prepare_dataset(dataset_train)\n",
    "dataset_valid = prepare_dataset(dataset_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully created the three subsets, *train*, *valid* and *test*. (Note that in the file system, only test is in a separate directory, as it was that way in the original database. Separating the other subsets would be an unnecessary operation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 69395 images (80.0%)\n",
      "valid: 17349 images (20.0%)\n",
      "test:  0 images (0.0%)\n"
     ]
    }
   ],
   "source": [
    "data_count_test = 0 # len(os.listdir(dir_test_path))\n",
    "images_count = data_count_train + data_count_valid + data_count_test\n",
    "\n",
    "print(f\"train: {data_count_train} images ({data_count_train / images_count * 100:.1f}%)\")\n",
    "print(f\"valid: {data_count_valid} images ({data_count_valid / images_count * 100:.1f}%)\")\n",
    "print(f\"test:  {data_count_test } images ({data_count_test  / images_count * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Rescaling\n",
    "# from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a model based on the VGG16 architecture, which became unstable for reasons we could not find out. We thus discontinued using the model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_shape = images_train.shape[1:]\n",
    "# kernel_size = (3, 3)\n",
    "# convolution_activation = None\n",
    "# convolution_padding = \"same\"\n",
    "# kernel_counts = [32, 64, 128, 128, 128]\n",
    "\n",
    "# base_model = Sequential()\n",
    "# base_model.add(Conv2D(kernel_counts[0], kernel_size, input_shape=image_shape, padding=convolution_padding, activation=convolution_activation))\n",
    "# base_model.add(Conv2D(kernel_counts[0], kernel_size, padding=convolution_padding))\n",
    "# base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# base_model.add(Conv2D(kernel_counts[1], kernel_size, padding=convolution_padding, activation=convolution_activation))\n",
    "# base_model.add(Conv2D(kernel_counts[1], kernel_size, padding=convolution_padding))\n",
    "# base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# base_model.add(Conv2D(kernel_counts[2], kernel_size, padding=convolution_padding, activation=convolution_activation))\n",
    "# base_model.add(Conv2D(kernel_counts[2], kernel_size, padding=convolution_padding))\n",
    "# base_model.add(Conv2D(kernel_counts[2], kernel_size, padding=convolution_padding))\n",
    "# base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# base_model.add(Conv2D(kernel_counts[3], kernel_size, padding=convolution_padding, activation=convolution_activation))\n",
    "# base_model.add(Conv2D(kernel_counts[3], kernel_size, padding=convolution_padding))\n",
    "# base_model.add(Conv2D(kernel_counts[3], kernel_size, padding=convolution_padding))\n",
    "# base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# base_model.add(Conv2D(kernel_counts[4], kernel_size, padding=convolution_padding, activation=convolution_activation))\n",
    "# base_model.add(Conv2D(kernel_counts[4], kernel_size, padding=convolution_padding))\n",
    "# base_model.add(Conv2D(kernel_counts[4], kernel_size, padding=convolution_padding))\n",
    "# base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# base_model.add(Flatten())\n",
    "# base_model.add(Dense(1000, activation=\"relu\"))\n",
    "# base_model.add(Dropout(0.5))\n",
    "# base_model.add(Dense(500, activation=\"relu\"))\n",
    "# base_model.add(Dropout(0.5))\n",
    "# base_model.add(Dense(labels_train.shape[1], activation=\"softmax\"))\n",
    "\n",
    "# base_model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided on using the original VGG16 model, with which, however, we couldn't train on our dataset either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use VGG16 as base model for transfer learning\n",
    "base_model = VGG16(\n",
    "    include_top=False,\n",
    "    input_shape=input_shape,\n",
    "    input_tensor=Input(shape=input_shape),\n",
    ")\n",
    "base_model.trainable = False # freeze the base model\n",
    "\n",
    "# Create a separate model for the appended layers, so that the trainable property can be set separately\n",
    "appended_model = Sequential(name=\"appended_model\")\n",
    "appended_model.add(GlobalAveragePooling2D(input_shape=base_model.output_shape[1:]))\n",
    "appended_model.add(Dense(1000, activation=\"relu\"))\n",
    "appended_model.add(Dropout(0.2))\n",
    "appended_model.add(Dense(100, activation=\"relu\"))\n",
    "appended_model.add(Dropout(0.2))\n",
    "appended_model.add(Dense(output_shape[0], activation=\"softmax\"))\n",
    "\n",
    "# Connect the base and appended models\n",
    "inputs = Input(shape=input_shape, name=\"main_model_input\")\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = appended_model(x)\n",
    "\n",
    "model = Model(inputs, x, name=\"main_model\")\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"main_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " main_model_input (InputLaye  [(None, 448, 448, 3)]    0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 448, 448, 3)      0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " tf.nn.bias_add (TFOpLambda)  (None, 448, 448, 3)      0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 14, 14, 512)       14714688  \n",
      "                                                                 \n",
      " appended_model (Sequential)  (None, 18)               614918    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,329,606\n",
      "Trainable params: 614,918\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried various batch sizes, optimizers, number of attached final dense layers with various sizes and activations, but our accuracy would barely exceed 0.06 even on the training data set. We were unable to tell why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  51/2168 [..............................] - ETA: 12:33:45 - loss: 2284.6646 - accuracy: 0.0619"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "model.fit(\n",
    "    dataset_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=dataset_valid,\n",
    "    steps_per_epoch=data_count_train // batch_size,\n",
    "    validation_steps=data_count_valid // batch_size,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our model did not learn as expected, evaluating its performance became obsolete. If we, however, figured out how the training process could avoid failing (which we definitely will!), we would add the following to our notebook:\n",
    "1. Load the test data set into memory and preprocess it similarly to the train and validation data sets.\n",
    "1. Execute our model on the test data set to see its final performance.\n",
    "1. Compute the confusion matrix of our model for all parameters, age, gender and race to see its performance broken down by each individual parameter.\n",
    "\n",
    "Then we would further experiment with how transfer learning could be done efficiently, as a final goal of our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for interpreting the output of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age(model_output):\n",
    "    model_output = np.atleast_2d(model_output)\n",
    "    return age_encoder.inverse_transform(model_output[:, :age_encoder.categories_[0].shape[0]]).reshape(-1)\n",
    "\n",
    "def get_gender(model_output):\n",
    "    model_output = np.atleast_2d(model_output)\n",
    "    return gender_encoder.inverse_transform(model_output[:, age_encoder.categories_[0].shape[0]:][:,gender_encoder.categories_[0].shape[0]]).reshape(-1)\n",
    "\n",
    "def get_race(model_output):\n",
    "    model_output = np.atleast_2d(model_output)\n",
    "    return race_encoder.inverse_transform(model_output[:, -race_encoder.categories_[0].shape[0]:]).reshape(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc1c9eb218cc5e25e848c84021af89099061b305f10f96c210bebd3d437c2ee0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
